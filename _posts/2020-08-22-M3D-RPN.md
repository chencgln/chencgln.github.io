---
layout: post
title:  "M3D-RPN: Monocular 3D Region Proposal Network for Object Detection"
date:   2020-08-22 18:00
categories: PaperReading 
tags: [3D Object Detection, Vision]
comments: true
toc: true
---

文章主要提出了同时输出 2D 和 3D proposal 的 RPN 网络，即 propose 的 anchor 将同时包含 2D 和 3D 参数信息。从而实现了端到端的 3D 检测框架。


* 构建了一个 2D 和 3D 检测网络的模型结构：monocular 3D region proposal network (M3D-RPN)，实现多任务的目标 proposal
* 提出了 depth-aware 的卷积来更好提取三维空间相关的特征，来提高 3D 参数的预测的效果；

## 算法流程

### 模型结构

![](https://glimg.oss-cn-shanghai.aliyuncs.com/test/20200821011502.png)

#### Anchor 设置

由于需要 RPN 同时提供 2D、3D 相关的 proposal，则定义 anchor 需要包含的参数有：

$$[w,h], z, [w,h,l,\theta]_{3D}$$

其中 $\theta$ 为目标的观察角。

利用 anchor 的中心点像素 $(u,v)$，可以结合相机内参 $P$ 和预测的深度 $z$ 得到目标的三维中心点坐标 $(x,y,z)$。

设 anchor 的数量为 $n_a$，待预测的类别数为 $n_c$，输出 feature map 的尺寸为 $h*w$。所有分配的 anchor 总数为 $h*w*n_a$，

#### Detection 模块

对 $h*w*n_a$ 个 anchor，每个进行 2D/3D 的各项参数进行预测。其中 2D BBox 相关的 $(x,y,w,h)$ 与常规的 2D 检测方式相同。

3D 相关的参数有：投影的中心点坐标 $(t_x,t_y,t_z)_P$，目标尺寸 $(t_w,t_h,t_l)$，及朝向角 $t_{\theta_{3D}}$

$$\begin{aligned}
&x'_P=x_P+t_{x_P}*w_{2D}, \quad && w'_{3D}=exp(t_{w_{3D}})*w_{3D} \\
&y'_P=y_P+t_{y_P}*h_{2D}, \quad && h'_{3D}=exp(t_{h_{3D}})*h_{3D} \\
&z'_P=t_{z_P}+z_{P}, \quad && l'_{3D}=exp(t_{l_{3D}})*l_{3D} \\
&\theta'_{3D} = t_{\theta_{3D}}+\theta_{3D}
\end{aligned}$$

其中，$x_P, y_P$ 为响应 anchor 的像素坐标，此处预测的目标 3D 中心点使用的是图像上的投影点，目的在更好的利用图像的特征。


#### 损失函数定义

通过 2D BBOx 的 IOU 获取获取目标 proposal 对应的 GT，损失函数有：

* 分类损失 $L_c$：Logistic loss
* 2D BBox 损失 $L_{b_2D}$：IOU Loss
* 3D 损失 $L_{b_3D}$：Smooth L1 loss
  
\* note：目标的 objectness 与分类的概率作为同一变量进行学习和预测，并根据阈值判断正负样本

#### Depth-aware Convolution

文中对 Depth-aware Convolution 的定义为：regular 2D convolution where a set of discretized depths are able to learn non-shared weights and features。

![](https://glimg.oss-cn-shanghai.aliyuncs.com/test/20200908152732.png)

定义了超参 $b$，表示将 feature map 划分成 $b$ 个与图像 raw-wise 的 bins。因驾驶场景的图像 y 轴与深度有较强的关系，对 $b$ 个 kernel 进行独立地学习，是的能够提取 3D 相关的场景特征。

#### 模型结构

最终的模型设计结构如图：

![](https://glimg.oss-cn-shanghai.aliyuncs.com/test/20200908161859.png)

经过 DenseNet 的得到 feature map，将特征图分别输入常规 2D 卷积、depth-aware 卷积两个分支，得到目标的各个参数。最后，将两个分支进行利用学习的权重进行加权，得到最后的结果。

## 模型表现

#### 3D AP

![](https://glimg.oss-cn-shanghai.aliyuncs.com/test/20200908162450.png)