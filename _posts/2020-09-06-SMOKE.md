---
layout: post
title:  "SMOKE: Single-Stage Monocular 3D Object Detection via Keypoint Estimation"
date:   2020-09-06 18:00
categories: PaperReading 
tags: [Object Detection, Vision]
comments: true
toc: true
---


文章提出，现有的目标 3D 检测模型，往往通过 RPN 进行目标的 Proposal，然后对 proposal 进行进一步的 3D 方面的参数预测。对于这一流程，无效的 proposal 在 3D 检测的过程中，影响了训练的效率和精度。因此，作者设计了单目的 one stage 网络，实现端到端的 3D 检测。

论文的主要贡献：

* 提出了端到端的单目 3D 检测模型，并达到了较高的精度；
* 对 3D 检测中不同子任务对应的参数进行了独立的训练（multi-step disentangling 损失），用于分离不同量纲的各项损失；
* 模型的表现达到了 SOTA，超过了 KITTI 上的其他单目 3D 检测方法。


### 算法和模型结构

* **Backbone**，使用了 DLA-34 作为 backbone 进行特征提取
* **Keppoint Branch**，1)如 CenterNet 一样，使用了中心点响应各个目标。论文使用的中心点是 3D 中心点的投影。训练过程中的标签是关于中心点的高斯核，高斯核的标准差根据数据集的统计得到；2)每个 3D 目标用其 8 个顶点的投影表示，并根据 8 个顶点可以得到目标的 2D BBOx
* **Regression Branch**，3D 回归的参数有：$(\delta_z,\delta_{x_c},\delta_{y_c},\delta_h,\delta_w,\delta_l,sin\theta,cos\theta)$，类似其他的 3D 检测方法，对坐标、dims预测了残差，对朝向使用了 $cos$ 和 $sin$进行预测；

note: 预测值 $[h,w,l]$，可以根据上述残差得到：

$$
\left[
\begin{matrix}
h \\ w \\ l
\end{matrix}
\right] = \left[
\begin{matrix}
\bar{h} e^{\delta_h} \\ \bar{w} e^{\delta_w} \\ \bar{l}e^{\delta_l}
\end{matrix}
\right]$$

### 损失函数


* **Keypoint 损失**，feature map 上各像素对应的值，与高斯核作用后的标签进行误差计算，损失使用了引入 focal loss 的交叉熵；
* **Regression 损失**，根据回归的 3D 相关的各个参数，可以计算目标的 3D 顶点。因此，使用各个顶点的 L1 loss，即可联合的优化各项任务。

Note. 
论文所表述任务独立拆分表现与：
1. $[x,y,z]$ 的预测使用了 GT 的像素坐标 $[x_c,y_c]$ 与预测的 $[\delta_z,\delta_{x_c},\delta_{y_c}]$ 求得；
2. 目标 $r_y$ 由预测的观察角 $\alpha$ 和 GT 的 $[x,y,z]$ 转换得到；

通过利用 GT，去除了预测值的互相依赖，最终的损失函数可以表示为：

$$L = L_{cls} + \sum_{i=1}^3 L_{reg}(\hat{B_i})$$

其中，$i$ 表示目标的 3D 角点分别独立来自于标的 $[h,w,l]$ 预测值、$[x,y,z]$ 或 $\alpha$ 预测值；实现了自任务的独立训练。


## 模型表现

验证集：

![](https://glimg.oss-cn-shanghai.aliyuncs.com/test/20200907102857.png)


测试集：
![](https://glimg.oss-cn-shanghai.aliyuncs.com/test/20200907102745.png)

#### Ablation Study

* 3D BBox 使用 L1 和 Smooth L1 损失函数的比较，第三行比较了 3D 任务分解得到的效果

![](https://glimg.oss-cn-shanghai.aliyuncs.com/test/20200907103251.png)