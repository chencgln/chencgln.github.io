---
layout: post
title:  "3DOP: 3D Object Proposals for Accurate Object Class Detection
"
date:   2020-07-12 18:00
categories: PaperReading 
tags: [Object Detection, Vision]
comments: true
toc: true
---

该论文的作者（Xiaozhi Chen）基于 3D Object Proposal 的流程，先后与 2015 和 2017 年发表了两篇论文，实现了2D、3D 的目标检测。

## 3DOP 2015

首先是 2015 发表的：[3D Object Proposals for Accurate Object Class Detection]()，文章针对 RCNN 在 KITTI 的自动驾驶场景下表现不足的问题，提出了基于 3D 线索的目标检测方法，并同时能够预测目标的 3D 相关参数。

对此，分析了 KITTI 小目标、目标遮挡较多，及光照影响较大的特点，文章提出了一种基于双目图像，在 3D 空间进行目标的 proposal 的目标检测方案，并同时使用了**三维尺寸先验**、**地平假设**，及包括**free space**、**点云**、**可见性**和**地面高度**相关深度方面的特征。

### 算法细节

#### 3D Proposal

3D Proposal （记为 $\bold{y}$）的表示：一个 3D 的 proposal 可以表示成 $(x,y,z,\theta,c,t)$，其中 $(x,y,z)$ 为中心点坐标，$(\theta,c,t)$ 分别表示目标朝向角、类别和该类别对应的尺寸模版。

同时，利用双目图像的得到的深度图([Yamaguchi]())，可以转换至点云，帮助构建对 3D Proposal 的约束 。


通过构建能量函数，对穷举的 3D Proposal 进行打分。具体的，模型使用了 MRF 来最小化能量函数，得到置信度较高的 3D 的 proposal。MRF 的能量函数为：

$$E(x,y)=\bold{w}_{c,pcd}^\top \phi_{pcd}(x,y)+\bold{w}_{c,fs}^\top \phi_{fs}(x,y)+\bold{w}_{c,ht}^\top \phi_{ht}(x,y)+\bold{w}_{c,ht-contr}^\top \phi_{ht-contr}(x,y)$$

其中，$\bold{w}$ 是学习得到的与类别相关权重，各项意义如下，

* $\phi_{pcd}(x,y)$ 表示了 box 中的点云的密度，通过统计 BBox 内部划分的 voxel 内的点云数量实现；
* $\phi_{fs}(x,y)$ 通过惩罚 free space 对应的点云与 3D proposal 有互相遮挡的情况；
* $\phi_{ht}(x,y)$ 用于控制目标内点云的高度范围，可以通过类别对应的目标高度先验进行约束；
* $\phi_{ht-contr}(x,y)$ 用于限制目标周围的点云的高度，该项基于目标周围的点需低于3D BBox 的假设（*存疑*）

##### Notes

* 使用 RANSAC 用于将图像像素与地面进行拟合
* 考虑远距离深度图的精度，20m 以外的目标，在高度方向穷举了额外的 Proposal

#### 检测模块

检测模块基于 Fast-RCNN 的结构，共享了各 ROI 的权重，并利用 ROI pooling 获取各个目标对应的特征。此外，在特征提取的网络后添加了 context 分支，获取目标 bbox 1.5 倍大小范围内的特征。

Context 模块与原始的 ROI 特征 Concat 后送入检测分支。同时，在原有的检测 Head（分类和BBox回归）以外，添加了使用 smooth L1 损失训练的朝向角分支。

### 结果展示

#### 指标

* AP

![](https://glimg.oss-cn-shanghai.aliyuncs.com/test/20200830192436.png)

* AOS

![](https://glimg.oss-cn-shanghai.aliyuncs.com/test/20200830192632.png)

#### 示例

![](https://glimg.oss-cn-shanghai.aliyuncs.com/test/20200830192123.png)
  

## 3DOP 2017

2017 年发表的 3DOP 的方法：[3D Object Proposals using Stereo Imagery for Accurate Object Class Detection]()，在 2015 提出的方法中提出了一些改进的方法，并得到了更高的检测精度，主要的工作包括：

1. 尝试了新的模型结构，将 RGB 图像和深度图按单分支和双分支的方式进行实验，充分利用了深度图提供的特征；
2. 尝试了真实的激光点云，对比了集中方法的性能；
3. 引入了 3D Recall 对模型进行了比较。（之前的工作都是评价 2D 检测的 AP 和朝向角 AOS）

### 算法细节

网络的模型结构如下，目前输入只是 RGB 图像

![](https://glimg.oss-cn-shanghai.aliyuncs.com/test/20200831013358.png)

#### Detection Head

Detection Head 包含两个部分，一个是与 3DOP-2015 相同的 2D 检测和朝向角预测的 Head；另外增加了 3D 检测的 Head。2D 和朝向角部分，与之前的一样。

3D 检测部分，使用了与上述图中 2D 相同的流程和结构，其回归的参数为 $P=(P_x,P_y,P_z,P_x^s,P_y^s,P_z^s)$，表示中心点坐标和三维尺寸。结合已有的朝向角预测分支，就能得到完整的 3D 检测。

#### More Depth Feature

为获取更多深度方面的信息，文章还额外使用了 HHA 特征，并将该特征作为输入，与图像同时提取特征用于检测。HHA 特征是对深度图进行编码得到的 3 通道数据，分别为：视差图、较地面的高度 $h$、各像素的法向量角度。

disparity map, height above the ground, and the angle of the normal at each pixel 

HHA 的使用有两种方式：

* 直接利用上述模型结构，将输入修改为 RGB 图像和深度图 concat 的 6 通道数据；

* 双分支 Backbone：图像和深度图进行独立的特征提取和 ROI Pooling，在多任务的 Head 前进行 concat 得到融合的特征

![](https://glimg.oss-cn-shanghai.aliyuncs.com/test/20200831013437.png)


### 实验结果


$AP_{2D}$:

![](https://glimg.oss-cn-shanghai.aliyuncs.com/test/20200831015810.png)

$AOS$:

![](https://glimg.oss-cn-shanghai.aliyuncs.com/test/20200831015856.png)



$AP_{3D}$: 此外，还引入了 $AP_{3D}$ 对不同的模型设计进行了比较，指标如下，

![](https://glimg.oss-cn-shanghai.aliyuncs.com/test/20200831020008.png)

